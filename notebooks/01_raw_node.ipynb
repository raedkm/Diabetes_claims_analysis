{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Data Ingestion](#toc1_)    \n",
    "- [Data Cleaning](#toc2_)    \n",
    "  - [Handling Missing Data and Duplicates Documentation](#toc2_1_)    \n",
    "- [Data Quality Checks Documentation](#toc3_)    \n",
    "    - [The data quality checks phase ensures the dataset is accurate, consistent, and logically sound for analysis.](#toc3_1_1_)    \n",
    "- [Data Quality Report](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Data Ingestion](#toc0_)\n",
    "\n",
    "<a id='toc1_1_1_'></a>[The loading stage focuses on ensuring data is correctly ingested with proper data types and meaningful column names.](#toc0_)\n",
    "\n",
    "**Best Practices Followed:**\n",
    "1. **Defining Data Types:**\n",
    "   - A `dtype_dict` is created to explicitly specify data types for each column in the dataset. This improves memory efficiency and ensures correct data interpretation.\n",
    "   - Examples include treating `gender` as a categorical variable and `bmi` as a float for precise numerical analysis.\n",
    "\n",
    "2. **Using a Column Lookup Table:**\n",
    "   - A dictionary (`column_lookup`) is used to rename columns to more descriptive and meaningful names. This makes the dataset easier to understand and work with.\n",
    "\n",
    "3. **Efficient Loading:**\n",
    "   - The dataset is loaded using `pd.read_csv` with the `dtype_dict`, minimizing post-load type conversions and errors.\n",
    "\n",
    "4. **Validation:**\n",
    "   - The data types and the first few rows are printed to verify successful loading and renaming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import data_path, list_data_files\n",
    "\n",
    "# Prin the list of data files\n",
    "print(list_data_files())\n",
    "\n",
    "# Define the path to the raw data file\n",
    "raw_data_path =  data_path()+ \"\\\\\" + 'raw\\\\sample_set_1 1-1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types and column renaming mappings\n",
    "dtype_dict = {\n",
    "    \"MEMBER_CODE\": \"int64\",\n",
    "    \"Age\": \"int64\",\n",
    "    \"GENDER\": \"category\",\n",
    "    \"POLICY_NO\": \"int64\",\n",
    "    \"CMS_Score\": \"int64\",\n",
    "    \"ICD_CODE\": \"string\",\n",
    "    \"ICD_desc\": \"string\",\n",
    "    \"City\": \"string\",\n",
    "    \"CLAIM_TYPE\": \"category\",\n",
    "    \"BMI\": \"float64\"\n",
    "}\n",
    "\n",
    "column_lookup = {\n",
    "    \"MEMBER_CODE\": \"member_code\",\n",
    "    \"Age\": \"age\",\n",
    "    \"GENDER\": \"gender\",\n",
    "    \"POLICY_NO\": \"policy_number\",\n",
    "    \"CMS_Score\": \"cms_score\",\n",
    "    \"ICD_CODE\": \"icd_code\",\n",
    "    \"ICD_desc\": \"icd_description\",\n",
    "    \"City\": \"city\",\n",
    "    \"CLAIM_TYPE\": \"claim_type\",\n",
    "    \"BMI\": \"bmi\"\n",
    "}\n",
    "\n",
    "# Load the dataset with specified data types and rename columns in one step\n",
    "raw_data = pd.read_csv(raw_data_path, dtype=dtype_dict).rename(columns=column_lookup)\n",
    "\n",
    "# Display data types and first few rows to confirm\n",
    "print(f\"Data types after loading:\\n{raw_data.dtypes}\\n\")\n",
    "print(\"First few rows of the data:\\n\", raw_data.head())\n",
    "\n",
    "# Save the cleaned data to Parquet format for efficient storage and type preservation\n",
    "raw_data.to_parquet(\"..\\\\data\\\\raw\\\\diabetic_claims.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_data.shape)\n",
    "display(raw_data.info())\n",
    "display(raw_data.head())\n",
    "display(raw_data.tail())\n",
    "display(raw_data.describe())\n",
    "display(raw_data.isnull().sum())\n",
    "display(raw_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayt the total null values in the dataset\n",
    "display(raw_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Data Cleaning](#toc0_)\n",
    "## <a id='toc2_1_'></a>[Handling Missing Data and Duplicates Documentation](#toc0_)\n",
    "\n",
    "1. **Identifying Missing Data:**\n",
    "   - A summary of missing values is generated to identify columns with missing entries.\n",
    "\n",
    "2. **Handling Missing Data:**\n",
    "   - Missing values in the `city` column are replaced with \"Unknown\" as an example strategy.\n",
    "      - there are `4700` missing values for `city`  \n",
    "   - Other strategies can include imputation or dropping rows/columns based on context.\n",
    "\n",
    "3. **Checking for Duplicates:**\n",
    "   - Duplicate rows are identified, counted, and removed to ensure data uniqueness and prevent bias. \n",
    "      - There are `2` duplicate rows\n",
    "   - Identified that `claim_type` is a duplicate of another row (I and O values). To test this we check if rows with an I value have a identical row with an O value\n",
    "      - Number of rows with 'I' value that have identical rows with 'O' value: 34233 out of 34235\n",
    "\n",
    "4. **Validation:**\n",
    "   - After handling missing data and duplicates, the data is re-checked to confirm integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values: summary and filling city missing values with 'Unknown'\n",
    "missing_summary = raw_data.isnull().sum()\n",
    "print(f\"Missing Values Summary:\\n{missing_summary}\\n\")\n",
    "\n",
    "# Fill missing city values with 'Unknown'\n",
    "raw_data['city'].fillna('Unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove duplicates\n",
    "duplicate_count = raw_data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Display duplicate rows if they exist, then remove duplicates\n",
    "if duplicate_count > 0:\n",
    "    print(\"Duplicate rows:\\n\", raw_data[raw_data.duplicated()])\n",
    "    raw_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verify changes after handling missing values and duplicates\n",
    "print(\"Data after handling missing values and duplicates:\\n\", raw_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DataFrame into two subsets: one with 'I' and one with 'O'\n",
    "df_I = raw_data[raw_data['claim_type'] == 'I']\n",
    "df_O = raw_data[raw_data['claim_type'] == 'O']\n",
    "\n",
    "# Merge the subsets on all columns except 'claim_type' to find identical rows\n",
    "common_columns = [col for col in raw_data.columns if col != 'claim_type']\n",
    "merged_df = pd.merge(df_I, df_O, on=common_columns, suffixes=('_I', '_O'))\n",
    "\n",
    "# Identify and count identical rows\n",
    "identical_rows = merged_df[common_columns]\n",
    "num_identical_rows = len(identical_rows)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Identical rows with 'I' and 'O' claim_type:\\n{identical_rows}\")\n",
    "print(f\"Number of identical rows: {num_identical_rows} out of {len(df_I)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Quality Checks Documentation](#toc0_)\n",
    "\n",
    "### <a id='toc3_1_1_'></a>[The data quality checks phase ensures the dataset is accurate, consistent, and logically sound for analysis.](#toc0_)\n",
    "\n",
    "**Steps to be Implemented:**\n",
    "\n",
    "1. **Logical Value Checks:**\n",
    "   - Validate values within logical ranges for key variables:\n",
    "     - `age`: Ensure all ages are within a plausible range (e.g., 0-120).\n",
    "     - `bmi`: Confirm all BMI values fall within a reasonable range (e.g., 10-80).\n",
    "     - `gender`: Ensure only valid values are present (e.g., \"M\", \"F\").\n",
    "\n",
    "2. **Consistency Check for Member Data:**\n",
    "   - Verify that variables that should not change across rows for a member (e.g., `member_code`, `gender`) are consistent.\n",
    "   - We identified that the member_count is does not represent a unique individual, this has been confirmed by conducting a member count grouped by policy_number, member_code, age and gender (show table). It is plausable that the member code represents a famliy unit with multiple individuals when applicable.\n",
    "   - This has been flagged to create a unique identifier for an individual.\n",
    "   - In addition to create a feature table indicating the family unit size across policy_number and member_code for further analysis. \n",
    "\n",
    "3. **ICD Code and Description Lookup:**\n",
    "   - Create a unique lookup table of `icd_code` and `icd_description` to identify duplicate or erroneous mappings.\n",
    "\n",
    "4. **Save Intermediate Quality Report:**\n",
    "   - Document any issues found during checks and save.\n",
    "\n",
    "5.  **Addressing the quality issues of the data:**\n",
    "   - The `non_unique_member_codes.csv` file documents the identified `member_code` issues.\n",
    "   - To adress this we will create a unique ID by grouping `policy_number`, `member_code`, `gender`, and `age` to differentiate separate individuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Logical checks for invalid age, BMI, and gender\n",
    "\n",
    "# Check for invalid age\n",
    "invalid_age_rows = raw_data[~raw_data['age'].between(0, 120)]\n",
    "if not invalid_age_rows.empty:\n",
    "    print(f\"\\nNumber of rows with invalid age values: {invalid_age_rows.shape[0]}\")\n",
    "    print(f\"Rows with invalid age values:\\n{invalid_age_rows[['age', 'member_code']]}\\n\")\n",
    "\n",
    "# Check for invalid BMI\n",
    "invalid_bmi_rows = raw_data[~raw_data['bmi'].between(10, 80)]\n",
    "if not invalid_bmi_rows.empty:\n",
    "    print(f\"\\nNumber of rows with extreme BMI values: {invalid_bmi_rows.shape[0]}\")\n",
    "    print(f\"Rows with extreme BMI values:\\n{invalid_bmi_rows[['bmi', 'member_code', 'policy_number']]}\\n\")\n",
    "    print(f\"Rows with extreme BMI values:\\n{invalid_bmi_rows[['bmi', 'member_code', 'policy_number']].drop_duplicates()}\\n\")\n",
    "    print(f\"Unique Rows with extreme BMI values:\\n{invalid_bmi_rows.drop_duplicates()}\\n\")\n",
    "\n",
    "# Check for invalid gender\n",
    "valid_genders = [\"M\", \"F\"]\n",
    "invalid_gender_rows = raw_data[~raw_data['gender'].isin(valid_genders)]\n",
    "if not invalid_gender_rows.empty:\n",
    "    print(f\"\\nNumber of rows with invalid gender values: {invalid_gender_rows.shape[0]}\")\n",
    "    print(f\"Rows with invalid gender values:\\n{invalid_gender_rows[['gender', 'member_code']]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Check for inconsistent gender in member_code\n",
    "inconsistent_gender_members = raw_data.groupby('member_code').filter(lambda group: group['gender'].nunique() > 1)\n",
    "\n",
    "# Display inconsistent rows\n",
    "if not inconsistent_gender_members.empty:\n",
    "    print(\"Inconsistent rows due to gender:\")\n",
    "    print(inconsistent_gender_members[['member_code', 'gender']].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Checking the frequency of member counts per policy number, member code, gender, and age\n",
    "member_table = raw_data[['policy_number', 'member_code', 'gender', 'age']].drop_duplicates()\n",
    "\n",
    "# Group by policy_number and member_code, then count the members\n",
    "counts_of_member = member_table.groupby(['policy_number', 'member_code']).size() \\\n",
    "    .value_counts().reset_index(name='frequency') \\\n",
    "    .rename(columns={'index': 'member_count'})\n",
    "\n",
    "# Display the result\n",
    "display(counts_of_member)\n",
    "\n",
    "# Identify and display duplicates based on member_code\n",
    "duplicates = member_table[member_table.duplicated(subset=['member_code'], keep=False)]\n",
    "display(duplicates.sort_values(by='member_code'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique ICD lookup table by selecting distinct icd_code and icd_description\n",
    "icd_lookup = raw_data[['icd_code', 'icd_description']].drop_duplicates()\n",
    "\n",
    "# Check for duplicate ICD codes\n",
    "duplicate_icd = icd_lookup['icd_code'].duplicated().sum()\n",
    "print(f\"Checking for duplicates: Number of duplicate ICD codes: {duplicate_icd}\")\n",
    "\n",
    "# Check for missing ICD descriptions\n",
    "missing_icd_desc = icd_lookup['icd_description'].isnull().sum()\n",
    "print(f\"Checking for missing values: Number of missing ICD descriptions: {missing_icd_desc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StringArray>\n",
       "[           'RIYADH',                <NA>,            'JEDDAH',\n",
       "          'ALKHOBAR',            'MAKKAH',              'HAIL',\n",
       "             'TABUK',            'MADINA',            'DAMMAM',\n",
       "              'TAIF',             'YANBU',          'AL-AHASA',\n",
       "            'ONAIZA',              'Abha',          'BUREIDAH',\n",
       "    'KHAMIS MUSHAIT',              'ARAR',            'SAKAKA',\n",
       "             'JIZAN',            'JUBAIL',            'KHAFJI',\n",
       "        'Al Quraiat',          'AL KHARJ',             'HOFUF',\n",
       "             'RAFHA',           'MAJMAAH',             'SIHAT',\n",
       "            'RABEGH',             'SAFWA',              'HAQL',\n",
       "      'HAFR ALBATEN',             'QATIF',        'BUKAIRIYAH',\n",
       "            'MAHAIL',             'DHEBA',           'AL BAHA',\n",
       "              'WAJH',            'NAJRAN',           'AL RUSS',\n",
       "             'ZULFI',            'BADAIE',             'SABYA',\n",
       "          'SHAROURA',              'AQIQ',         'AL DWADMI',\n",
       "     'SABT AL-ALAYA',              'DARB',      'Al Nuairiyah',\n",
       "            'TAROOT',             'AFLAJ',              'AFIF',\n",
       "   'WADI AL DAWASIR',          'SULAYYEL',            'Ibqaiq',\n",
       "        'AL KHORMAH',             'BAGAA',          'TABARJAL',\n",
       "             'BADER', 'HOUTAT BANI-TAMIM',           'METHNAB',\n",
       "       'RAS TANNURA',         'QUAIEYYAH',           'OYAYNAH',\n",
       "   'RIYADH AL-KABRA']\n",
       "Length: 64, dtype: string"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Quality Report](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy types to native Python types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, (np.int64, np.float64)):\n",
    "        return int(obj)\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Create the quality report data\n",
    "quality_issues = {\n",
    "    \"Duplicate ICD Codes\": int(duplicate_icd),\n",
    "    \"Missing ICD Descriptions\": int(missing_icd_desc),\n",
    "    \"Invalid/Extreme Age Rows\": invalid_age_rows[['age', 'member_code']].to_dict(orient='records') if not invalid_age_rows.empty else [],\n",
    "    \"Invalid/Extreme BMI Rows\": invalid_bmi_rows[['bmi', 'member_code']].to_dict(orient='records') if not invalid_bmi_rows.empty else [],\n",
    "    \"Members code with inconsistent Gender\": inconsistent_gender_members.index.tolist() if not inconsistent_gender_members.empty else [],\n",
    "}\n",
    "\n",
    "# Save the quality issues to a JSON file\n",
    "report_path = '..\\\\reports\\\\raw_data_quality_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(quality_issues, f, indent=4, default=convert_to_serializable)\n",
    "\n",
    "print(f\"Quality report saved as {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the raw data for intermediate processing\n",
    "intermediate_data = raw_data.copy()\n",
    "\n",
    "# Specify the path to save the intermediate data\n",
    "intermediate_data_path = f\"{data_path()}\\\\intermediate\\\\intermediate_data.parquet\"\n",
    "\n",
    "# Save the intermediate data to a Parquet file\n",
    "intermediate_data.to_parquet(intermediate_data_path, index=False)\n",
    "\n",
    "print(f\"Intermediate data saved to {intermediate_data_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
