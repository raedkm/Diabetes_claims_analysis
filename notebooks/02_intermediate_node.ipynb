{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils import data_path\n",
    "# specify the path to save the intermediate data\n",
    "intermediate_data_path = data_path() + '\\\\intermediate'+'\\\\intermediate_data.parquet'\n",
    "\n",
    "# Save the intermediate data to a parquet file\n",
    "intermediate_data = pd.read_parquet(intermediate_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Data Cleaning ](#toc1_)    \n",
    "  - 1.1. [Convert the claim_type to indicator and remvoving the duplicate value](#toc1_1_)    \n",
    "  - 1.2. [Assigning unique identifiers](#toc1_2_)    \n",
    "  - 1.3. [Standarizing city names](#toc1_3_)    \n",
    "- 2. [Feature engineering](#toc2_)    \n",
    "  - 2.1. [Feature engineering (new variables)](#toc2_1_)    \n",
    "    - 2.1.1. [Age, BMI and Obesity categories](#toc2_1_1_)    \n",
    "    - 2.1.2. [Unique identifiers](#toc2_1_2_)    \n",
    "  - 2.2. [Feature engineering (new tables)](#toc2_2_)    \n",
    "    - 2.2.1. [Diabetes type feature table](#toc2_2_1_)    \n",
    "    - 2.2.2. [Diabetes complications feature table](#toc2_2_2_)    \n",
    "    - 2.2.3. [Comorbidity feature table](#toc2_2_3_)    \n",
    "    - 2.2.4. [Family size feature table](#toc2_2_4_)    \n",
    "    - 2.2.5. [Unique identifier feature table](#toc2_2_5_)    \n",
    "- 3. [Create Lookup Tables](#toc3_)    \n",
    "  - 3.1. [Standarized city name lookup table](#toc3_1_)    \n",
    "  - 3.2. [ICD 10 code lookup table](#toc3_2_)    \n",
    "- 4. [Saving the primary data](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Data Cleaning](#toc1_)  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Convert the claim_type to indicator and remvoving the duplicate value](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_indicator_and_remove_duplicates(data, column):\n",
    "    \"\"\"\n",
    "    Convert a specified column to indicator columns and remove redundant duplicate rows.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The column name to convert to indicator columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with indicator columns and duplicates removed.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # Create indicator columns for the specified column values\n",
    "    unique_values = data[column].unique()\n",
    "    for value in unique_values:\n",
    "        indicator_column_name = f\"{column}_{value}\"\n",
    "        data[indicator_column_name] = (data[column] == value).astype(int)\n",
    "\n",
    "    # Remove the original column\n",
    "    data = data.drop(columns=[column])\n",
    "\n",
    "    # Drop duplicate rows based on all columns except the new indicator columns\n",
    "    common_columns = [col for col in data.columns if not col.startswith(f\"{column}_\")]\n",
    "    data = data.drop_duplicates(subset=common_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Convert the 'claim_type' column to indicator columns and remove redundant duplicate rows\n",
    "intermediate_data = convert_to_indicator_and_remove_duplicates(intermediate_data, 'claim_type').drop(columns=['claim_type_O'])\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(\"Updated DataFrame with claim_type indicators and duplicates removed:\")\n",
    "display(intermediate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Assigning unique identifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assigning unique ID to individuals\n",
    "\n",
    "def assign_unique_id(intermediate_data):\n",
    "    \"\"\"\n",
    "    Assigns a unique ID to individuals by grouping based on policy_number, member_code, gender, and age.\n",
    "\n",
    "    Parameters:\n",
    "    intermediate_data (DataFrame): The DataFrame containing the intermediate data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with an additional column 'unique_id'.\n",
    "    \"\"\"\n",
    "    intermediate_data['unique_id'] = (\n",
    "        intermediate_data.groupby(['policy_number', 'member_code', 'gender', 'age'],  observed=True)\n",
    "        .ngroup()\n",
    "        .astype(str)  # Convert the unique ID to a string\n",
    "\n",
    "    )\n",
    "    return intermediate_data\n",
    "\n",
    "# Assign a unique ID for individuals by grouping\n",
    "intermediate_data = assign_unique_id(intermediate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Standarizing city names](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standarize city names\n",
    "def standarize_city_name(name):\n",
    "    \"\"\"\n",
    "    standarize city names to ensure consistent formatting.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The original city name.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and standarized city name.\n",
    "        - Strips leading and trailing whitespace.\n",
    "        - Capitalizes the first letter of each word and handles hyphens appropriately.\n",
    "    \"\"\"\n",
    "    name = name.strip()  # Strip leading and trailing whitespace\n",
    "    # Split by spaces and hyphens, capitalize each part, and rejoin\n",
    "    parts = re.split(r'(\\s+|-)', name)\n",
    "    name = ''.join(part.capitalize() if part.isalpha() else part for part in parts)\n",
    "    return name\n",
    "\n",
    "\n",
    "# Normalize city names in the raw_data DataFrame\n",
    "intermediate_data['clean_city'] = intermediate_data['city'].apply(standarize_city_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[Feature engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. <a id='toc2_1_'></a>[Feature engineering (new variables)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. <a id='toc2_1_1_'></a>[Age, BMI and Obesity categories](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Age and BMI Categories\n",
    "age_bins = range(0, 121, 10)\n",
    "age_labels = [f\"{i}-{i+9}\" for i in range(0, 120, 10)]\n",
    "bmi_bins = [-float('inf'), 18.5, 25, 30, float('inf')]\n",
    "bmi_labels = ['Underweight', 'Healthy', 'Overweight', 'Obesity']\n",
    "obesity_bins = [-float('inf'), 30, 35, 40, float('inf')]\n",
    "obesity_labels = ['Not Obese', 'Class 1 Obesity', 'Class 2 Obesity', 'Class 3 Obesity']\n",
    "\n",
    "# Create categorical columns\n",
    "intermediate_data['age_cat'] = pd.cut(intermediate_data['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "intermediate_data['bmi_cat'] = pd.cut(intermediate_data['bmi'], bins=bmi_bins, labels=bmi_labels, right=False)\n",
    "intermediate_data['obesity_cat'] = pd.cut(intermediate_data['bmi'], bins=obesity_bins, labels=obesity_labels, right=False)\n",
    "\n",
    "# Print category distributions\n",
    "for col in ['age_cat', 'bmi_cat', 'obesity_cat']:\n",
    "    display(f\"{col} Distribution:\")\n",
    "    display(intermediate_data[col].value_counts())\n",
    "    display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. <a id='toc2_1_2_'></a>[Unique identifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique 'unique_id' values per 'clean_city'\n",
    "unique_id_counts_by_city = (intermediate_data.groupby('clean_city')['unique_id']\n",
    "                            .nunique()\n",
    "                            .reset_index(name='unique_id_count')\n",
    "                            .sort_values('unique_id_count', ascending=False))\n",
    "\n",
    "# Get the top 5 cities by unique_id count, excluding 'Unknown'\n",
    "top_5_cities = unique_id_counts_by_city.loc[unique_id_counts_by_city['clean_city'] != 'Unknown', 'clean_city'].head(5)\n",
    "\n",
    "# Create the 'major_city' variable\n",
    "intermediate_data['major_city'] = intermediate_data['clean_city'].apply(\n",
    "    lambda x: x if x in top_5_cities.values else 'Other'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. <a id='toc2_2_'></a>[Feature engineering (new tables)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate feature tables\n",
    "def generate_feature_table(data, condition, group_by_columns, agg_func='size'):\n",
    "    filtered_data = data[condition]\n",
    "    return filtered_data.pivot_table(index='unique_id', columns=group_by_columns, aggfunc=agg_func, fill_value=0).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. <a id='toc2_2_1_'></a>[Diabetes type feature table](#toc0_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create disease-related feature tables\n",
    "intermediate_data['icd_code_major'] = intermediate_data['icd_code'].str.split('.').str[0]\n",
    "disease_table = intermediate_data[['unique_id', 'icd_code_major', 'icd_code']].drop_duplicates()\n",
    "disease_icd_major = disease_table[['unique_id','icd_code_major']].drop_duplicates()\n",
    "\n",
    "# Diabetes type feature table\n",
    "diabetes_type_feature = generate_feature_table(disease_icd_major, disease_icd_major['icd_code_major'].str.contains('E'), 'icd_code_major')\n",
    "\n",
    "# Save the diabetes_type_feature table to a parquet file\n",
    "diabetes_type_feature.to_parquet(data_path() + '\\\\feature_store'+'\\\\diabetes_type_feature.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_type_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. <a id='toc2_2_2_'></a>[Diabetes complications feature table](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes complications feature table\n",
    "diabetes_complication_feature = generate_feature_table(\n",
    "    disease_table, \n",
    "    disease_table['icd_code'].str.contains('E') & disease_table['icd_code'].str.contains('\\.'), \n",
    "    'icd_code'\n",
    ")\n",
    "diabetes_complication_feature['total_complications'] = diabetes_complication_feature.drop(columns=['unique_id']).sum(axis=1).astype(int)\n",
    "\n",
    "# Save the diabetes complications feature table\n",
    "diabetes_complication_feature.to_parquet(data_path() + '\\\\feature_store'+'\\\\diabetes_complication_feature.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. <a id='toc2_2_3_'></a>[Comorbidity feature table](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comorbidities feature table\n",
    "comorbidity_feature = generate_feature_table(disease_icd_major, ~disease_icd_major['icd_code_major'].str.contains('E'), 'icd_code_major')\n",
    "comorbidity_feature['total_comorbidities'] = comorbidity_feature.drop(columns=['unique_id']).sum(axis=1).astype(int)\n",
    "\n",
    "# Save the comorbidity feature table to a parquet file\n",
    "comorbidity_feature.to_parquet(data_path() + '\\\\feature_store'+'\\\\comorbidity_feature.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. <a id='toc2_2_4_'></a>[Family size feature table](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fanmily size feature table\n",
    "def create_family_size_feature(data):\n",
    "    \"\"\"\n",
    "    Create a 'family_size' feature by the count of unique identifiers per 'policy_number' and 'member_code'.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with a 'family_size' feature.\n",
    "    \"\"\"\n",
    "    family_size_data = data.groupby(['policy_number', 'member_code'])['unique_id'].nunique().reset_index(name='family_size')\n",
    "    return family_size_data\n",
    "\n",
    "# usage of the create_family_size_feature function\n",
    "family_size_table = create_family_size_feature(intermediate_data)\n",
    "\n",
    "display(family_size_table)\n",
    "\n",
    "# saving the family size table\n",
    "family_size_table.to_parquet('..\\\\data\\\\feature_store\\\\family_size_table.parquet', index=False)\n",
    "family_size_table.to_csv('..\\\\data\\\\feature_store\\\\family_size_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. <a id='toc2_2_5_'></a>[Unique identifier feature table](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an identifier table with unique records\n",
    "identifier_table = intermediate_data[['unique_id', 'policy_number', 'member_code', 'age_cat', 'age', 'gender']].drop_duplicates()\n",
    "\n",
    "# Extract the maximum BMI and BMI categories for each unique_id\n",
    "max_bmi = intermediate_data.groupby('unique_id')['bmi'].max().reset_index(name='max_bmi')\n",
    "max_bmi_cat = intermediate_data.groupby('unique_id')['bmi_cat'].max().reset_index(name='max_bmi_cat')\n",
    "\n",
    "# Determine the city with the maximum count per unique_id\n",
    "max_city = intermediate_data.groupby(['unique_id', 'major_city']).size() \\\n",
    "                            .reset_index(name='count') \\\n",
    "                            .loc[lambda x: x.groupby('unique_id')['count'].idxmax()] \\\n",
    "                            .drop(columns=['count']) \\\n",
    "                            .rename(columns={'major_city': 'max_major_city'})\n",
    "\n",
    "# Merge max BMI,  BMI category, and max city into the identifier table\n",
    "identifier_table = identifier_table.merge(max_bmi, on='unique_id', how='left') \\\n",
    "                                   .merge(max_bmi_cat, on='unique_id', how='left') \\\n",
    "                                   .merge(max_city, on='unique_id', how='left')\n",
    "\n",
    "# Save to Parquet and CSV\n",
    "identifier_table.to_parquet('..\\\\data\\\\feature_store\\\\identifier_table.parquet', index=False)\n",
    "identifier_table.to_csv('..\\\\data\\\\feature_store\\\\identifier_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6. <a id='toc2_2_5_'></a>[city complications feature table](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Step 2: Create the city complications data set\n",
    "##\n",
    "\n",
    "# Merge the feature tables\n",
    "city_comp_df = identifier_table.merge(diabetes_type_feature, on = 'unique_id', how = 'left')\\\n",
    "                        .merge(diabetes_complication_feature[['unique_id', 'total_complications']], on = 'unique_id', how = 'left')\\\n",
    "                        .merge(comorbidity_feature[['unique_id','total_comorbidities']], on = 'unique_id', how = 'left')\n",
    "\n",
    "# Fill missing values with 0\n",
    "columns_to_fill = city_comp_df.loc[:, 'E09':'total_comorbidities'].columns\n",
    "city_comp_df[columns_to_fill] = city_comp_df[columns_to_fill].fillna(0)\n",
    "\n",
    "city_comp_df['has_icd_dm'] = city_comp_df.loc[:, 'E09':'E14'].gt(0).any(axis=1).astype(int)\n",
    "city_comp_df['total_dm_icd'] = city_comp_df.loc[:, 'E09':'E14'].sum(axis=1).astype(int)\n",
    "\n",
    "# Cleaning the data set\n",
    "# since this is a data set of patient with diabetes if there is not code for diabetes then we can assign the type as unspecified diabetes mellitus (E14)\n",
    "city_comp_df['total_complications'] = city_comp_df['total_complications'].astype(int)\n",
    "city_comp_df['total_comorbidities'] = city_comp_df['total_comorbidities'].astype(int)\n",
    "\n",
    "# assiging the type of diabetes to unspecified diabetes mellitus (E14) if there is no code for diabetes\n",
    "city_comp_df.loc[city_comp_df['total_dm_icd'] < 1, 'E14'] = 1\n",
    "\n",
    "# save the data set to the feature store\n",
    "city_comp_df.to_parquet('..\\\\data\\\\feature_store\\\\city_comp_df.parquet', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_comp_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[Create Lookup Tables](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. <a id='toc3_1_'></a>[Standarized city name lookup table](#toc0_)\n",
    "## 3.2. <a id='toc3_2_'></a>[ICD 10 code lookup table](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lookup table for the data\n",
    "\n",
    "# Create a lookup table\n",
    "city_lookup = (\n",
    "    intermediate_data.groupby('clean_city')['city']\n",
    "    .unique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'city': 'list_of_variants'})\n",
    ")\n",
    "\n",
    "# Save the lookup table to a CSV file\n",
    "city_lookup.to_csv('..\\\\data\\\\lookup\\\\city_lookup.csv', index=False)\n",
    "\n",
    "# Verify the lookup table\n",
    "print(\"\\nCity Lookup Table:\\n\", city_lookup)\n",
    "\n",
    "# Create and save the ICD lookup table\n",
    "icd_lookup = intermediate_data[['icd_code', 'icd_description']].drop_duplicates()\n",
    "\n",
    "# Save the lookup table to a CSV file\n",
    "icd_lookup.to_csv('..\\\\data\\\\lookup\\\\icd_lookup.csv', index=False)\n",
    "\n",
    "# Verify the lookup table\n",
    "print(\"\\nICD Code Lookup Table:\\n\", icd_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[Saving the primary data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data to a parquet file\n",
    "primary_data = intermediate_data.drop(columns=['city'])\n",
    "primary_data.rename(columns={'clean_city': 'city'}, inplace=True)\n",
    "\n",
    "primary_data.to_parquet('..\\\\data\\\\primary\\\\primary_data.parquet', index=False)\n",
    "primary_data.to_csv('..\\\\data\\\\primary\\\\primary_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
