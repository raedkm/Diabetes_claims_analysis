{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils import data_path\n",
    "# specify the path to save the intermediate data\n",
    "intermediate_data_path = data_path() + '\\\\intermediate'+'\\\\intermediate_data.parquet'\n",
    "\n",
    "# Save the intermediate data to a parquet file\n",
    "intermediate_data = pd.read_parquet(intermediate_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation and Feature creation\n",
    "\n",
    "1. **Creating data features**\n",
    "   - unique identifier (using policy number, member code, age, and gender)\n",
    "   - numerical to categorical\n",
    "      - age -> using 10 year intervals\n",
    "      - bmi_cat -> logical groups:\n",
    "         - underweight [<18.5], \n",
    "         - healthy [18.5 to <25], \n",
    "         - overweight [25 to <30], \n",
    "         - obesity [=> 30]\n",
    "   - Creating an obesity_cat  \n",
    "      - class 1 [30 to <35]\n",
    "      - class 2 [35 to <40]\n",
    "      - class 3 [=>40]\n",
    "   - Major city (top 5 city by unique identifier count)\n",
    "\n",
    "2. **Normalization Process:**\n",
    "Cleaning city names documentation\n",
    "\n",
    "The cleaning city names stage focuses on ensuring consistency and standardization in the dataset by processing city names. This involves handling variations in formatting, capitalization, and spelling. Consistent city names are crucial for accurate analysis, grouping, and reporting.\n",
    "\n",
    "   - A function `normalize_city_name` is created to handle city name inconsistencies. This function:\n",
    "     - Strips leading and trailing whitespace.\n",
    "     - Converts names to title case (e.g., \"new york\" becomes \"New York\").\n",
    "     - Properly formats hyphenated names (e.g., \"los-angeles\" becomes \"Los-Angeles\").\n",
    "     - Create a lookup table and store it for reuse in downstream processes and documentation purposes.\n",
    "\n",
    "3. **Converting Claims type into indicator**\n",
    "   -  A fucntion `convert_to_indicator_and_remove_duplicates` is created to handle the conversion of the claim type with 'I' values to and indicator and removing the duplicate rows.\n",
    "\n",
    "4. **Extracting Feature Tables**\n",
    "   - creating diabetes type, comorbidity, and diabetes feature tables by unique_id\n",
    "   - create fanmily size table\n",
    "   - unique identifier table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating categorical variables for `age` and `bmi`\n",
    "\n",
    "## Feature Engineering: Age and BMI Groups\n",
    "\n",
    "# Create age groups\n",
    "intermediate_data['age_cat'] = pd.cut(\n",
    "    intermediate_data['age'],\n",
    "    bins=range(0, 121, 10),  # 10-year intervals\n",
    "    labels=[f\"{i}-{i+9}\" for i in range(0, 120, 10)],\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Create BMI categories\n",
    "intermediate_data['bmi_cat'] = pd.cut(\n",
    "    intermediate_data['bmi'],\n",
    "    bins=[-float('inf'), 18.5, 25, 30, float('inf')],\n",
    "    labels=['Underweight', 'Healthy', 'Overweight', 'Obesity'],\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Create Obesity categories\n",
    "intermediate_data['obesity_cat'] = pd.cut(\n",
    "    intermediate_data['bmi'],\n",
    "    bins=[-float('inf'), 30, 35, 40, float('inf')],\n",
    "    labels=['Not Obese','Class 1 Obesity', 'Class 2 Obesity', 'Class 3 Obesity'],\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Check the distribution of the newly created categories\n",
    "print(\"Age Categories Distribution:\")\n",
    "print(intermediate_data['age_cat'].value_counts())\n",
    "\n",
    "print(\"BMI Categories Distribution:\")\n",
    "print(intermediate_data['bmi_cat'].value_counts())\n",
    "\n",
    "print(\"Obesity Categories Distribution:\")\n",
    "print(intermediate_data['obesity_cat'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assigning unique ID to individuals\n",
    "\n",
    "def assign_unique_id(intermediate_data):\n",
    "    \"\"\"\n",
    "    Assigns a unique ID to individuals by grouping based on policy_number, member_code, gender, and age.\n",
    "\n",
    "    Parameters:\n",
    "    intermediate_data (DataFrame): The DataFrame containing the intermediate data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with an additional column 'unique_id'.\n",
    "    \"\"\"\n",
    "    intermediate_data['unique_id'] = (\n",
    "        intermediate_data.groupby(['policy_number', 'member_code', 'gender', 'age'],  observed=True)\n",
    "        .ngroup()\n",
    "        .astype(str)  # Convert the unique ID to a string\n",
    "\n",
    "    )\n",
    "    return intermediate_data\n",
    "\n",
    "# Assign a unique ID for individuals by grouping\n",
    "intermediate_data = assign_unique_id(intermediate_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize city names\n",
    "def normalize_city_name(name):\n",
    "    \"\"\"\n",
    "    Normalize city names to ensure consistent formatting.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The original city name.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized city name.\n",
    "        - Strips leading and trailing whitespace.\n",
    "        - Capitalizes the first letter of each word and handles hyphens appropriately.\n",
    "    \"\"\"\n",
    "    name = name.strip()  # Strip leading and trailing whitespace\n",
    "    # Split by spaces and hyphens, capitalize each part, and rejoin\n",
    "    parts = re.split(r'(\\s+|-)', name)\n",
    "    name = ''.join(part.capitalize() if part.isalpha() else part for part in parts)\n",
    "    return name\n",
    "\n",
    "\n",
    "# Normalize city names in the raw_data DataFrame\n",
    "intermediate_data['clean_city'] = intermediate_data['city'].apply(normalize_city_name)\n",
    "\n",
    "# Create a lookup table\n",
    "city_lookup = (\n",
    "    intermediate_data.groupby('clean_city')['city']\n",
    "    .unique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'city': 'list_of_variants'})\n",
    ")\n",
    "\n",
    "# Save the lookup table to a CSV file\n",
    "city_lookup.to_csv('..\\\\data\\\\lookup\\\\city_lookup.csv', index=False)\n",
    "\n",
    "# Verify the lookup table\n",
    "print(\"\\nCity Lookup Table:\\n\", city_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_indicator_and_remove_duplicates(data, column):\n",
    "    \"\"\"\n",
    "    Convert a specified column to indicator columns and remove redundant duplicate rows.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The column name to convert to indicator columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with indicator columns and duplicates removed.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # Create indicator columns for the specified column values\n",
    "    unique_values = data[column].unique()\n",
    "    for value in unique_values:\n",
    "        indicator_column_name = f\"{column}_{value}\"\n",
    "        data[indicator_column_name] = (data[column] == value).astype(int)\n",
    "\n",
    "    # Remove the original column\n",
    "    data = data.drop(columns=[column])\n",
    "\n",
    "    # Drop duplicate rows based on all columns except the new indicator columns\n",
    "    common_columns = [col for col in data.columns if not col.startswith(f\"{column}_\")]\n",
    "    data = data.drop_duplicates(subset=common_columns)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Convert the 'claim_type' column to indicator columns and remove redundant duplicate rows\n",
    "intermediate_data_converted_data = convert_to_indicator_and_remove_duplicates(intermediate_data, 'claim_type').drop(columns=['claim_type_O'])\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(\"Updated DataFrame with claim_type indicators and duplicates removed:\")\n",
    "display(intermediate_data_converted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the icd code lookup table\n",
    "icd_lookup = intermediate_data[['icd_code', 'icd_description']].drop_duplicates()\n",
    "icd_lookup.to_csv('..\\\\data\\\\lookup\\\\icd_lookup.csv', index=False) \n",
    "\n",
    "# creating the diabetes type, diabetes complication, and comorbidity feature tables\n",
    "intermediate_data['icd_code_major'] = intermediate_data['icd_code'].str.split('.').str[0]\n",
    "disease_table = intermediate_data[['unique_id', 'icd_code_major', 'icd_code']].drop_duplicates()\n",
    "major_disease_table = disease_table[['unique_id', 'icd_code_major']].drop_duplicates()\n",
    "\n",
    "# diabetes type\n",
    "diabetes_type_table = major_disease_table[major_disease_table['icd_code_major'].str.contains('E')]\n",
    "diabetes_type_feature = diabetes_type_table.pivot_table(index='unique_id', columns='icd_code_major',  aggfunc='size', fill_value=0).reset_index()\n",
    "diabetes_type_feature.columns.name = None  # Remove the columns name\n",
    "\n",
    "# diabetes complications\n",
    "complication_table = disease_table[['unique_id', 'icd_code']].drop_duplicates()\n",
    "diabetes_complication_table = complication_table[complication_table['icd_code'].str.contains('E') & complication_table['icd_code'].str.contains('\\.')]\n",
    "diabetes_complication_feature = diabetes_complication_table.pivot_table(index='unique_id', columns='icd_code', aggfunc='size', fill_value=0).reset_index()\n",
    "diabetes_complication_feature.columns.name = None  # Remove the columns name\n",
    "diabetes_complication_feature['total_complications'] = diabetes_complication_feature.drop(columns=['unique_id']).sum(axis=1)\n",
    "\n",
    "# comorbidities\n",
    "comorbidity_type_table = major_disease_table[~major_disease_table['icd_code_major'].str.contains('E')]\n",
    "comorbidity_feature = comorbidity_type_table.pivot_table(index='unique_id', columns='icd_code_major',  aggfunc='size', fill_value=0).reset_index()\n",
    "comorbidity_feature.columns.name = None  # Remove the columns name\n",
    "comorbidity_feature['total_comorbidities'] = comorbidity_feature.drop(columns=['unique_id']).sum(axis=1)\n",
    "\n",
    "# save the feature tables\n",
    "diabetes_type_feature.to_parquet('..\\\\data\\\\feature_store\\\\diabetes_type_feature.parquet', index=False)\n",
    "diabetes_complication_feature.to_parquet('..\\\\data\\\\feature_store\\\\diabetes_complication_feature.parquet', index=False)\n",
    "comorbidity_feature.to_parquet('..\\\\data\\\\feature_store\\\\comorbidity_feature.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fanmily size feature table\n",
    "def create_family_size_feature(data):\n",
    "    \"\"\"\n",
    "    Create a 'family_size' feature by the count of unique identifiers per 'policy_number' and 'member_code'.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): The input DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with a 'family_size' feature.\n",
    "    \"\"\"\n",
    "    family_size_data = data.groupby(['policy_number', 'member_code'])['unique_id'].nunique().reset_index(name='family_size')\n",
    "    return family_size_data\n",
    "\n",
    "# usage of the create_family_size_feature function\n",
    "family_size_table = create_family_size_feature(intermediate_data_converted_data)\n",
    "display(family_size_table)\n",
    "family_size_table['family_size'].hist()\n",
    "family_size_table['family_size'].value_counts()\n",
    "\n",
    "# saving the family size table\n",
    "family_size_table.to_parquet('..\\\\data\\\\feature_store\\\\family_size_table.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an identifier table\n",
    "identifier_table = intermediate_data_converted_data[['unique_id', 'policy_number', 'member_code']].drop_duplicates()\n",
    "identifier_table.to_parquet('..\\\\data\\\\feature_store\\\\identifier_table.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>member_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3183</td>\n",
       "      <td>26730932</td>\n",
       "      <td>1961848012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3184</td>\n",
       "      <td>26730932</td>\n",
       "      <td>8238702512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3185</td>\n",
       "      <td>26730932</td>\n",
       "      <td>8638816522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3186</td>\n",
       "      <td>26730932</td>\n",
       "      <td>9534292522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3187</td>\n",
       "      <td>26730932</td>\n",
       "      <td>9762026522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173723</th>\n",
       "      <td>3309</td>\n",
       "      <td>26730932</td>\n",
       "      <td>279082000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173729</th>\n",
       "      <td>3310</td>\n",
       "      <td>26730932</td>\n",
       "      <td>284042000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173733</th>\n",
       "      <td>3311</td>\n",
       "      <td>26730932</td>\n",
       "      <td>288071000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173736</th>\n",
       "      <td>3312</td>\n",
       "      <td>26730932</td>\n",
       "      <td>288102000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173760</th>\n",
       "      <td>3313</td>\n",
       "      <td>26730932</td>\n",
       "      <td>292060000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id  policy_number      member_code\n",
       "0           3183       26730932       1961848012\n",
       "8           3184       26730932       8238702512\n",
       "12          3185       26730932       8638816522\n",
       "13          3186       26730932       9534292522\n",
       "15          3187       26730932       9762026522\n",
       "...          ...            ...              ...\n",
       "173723      3309       26730932  279082000000000\n",
       "173729      3310       26730932  284042000000000\n",
       "173733      3311       26730932  288071000000000\n",
       "173736      3312       26730932  288102000000000\n",
       "173760      3313       26730932  292060000000000\n",
       "\n",
       "[18694 rows x 3 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identifier_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data to a parquet file\n",
    "primary_data = intermediate_data_converted_data.drop(columns=['city'])\n",
    "primary_data.rename(columns={'clean_city': 'city'}, inplace=True)\n",
    "\n",
    "primary_data.to_parquet('..\\\\data\\\\primary\\\\primary_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other code not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features and storing them in the feature store\n",
    "\n",
    "## Using encoding (to remove order)\n",
    "use the funciton get_dummies for all cities, for major cities we replace all cities ranked >4 as other first\n",
    "- City encoding \n",
    "- Top Major cities (4  + 'other') encoding\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city  unique_id_count\n",
      "28           Jeddah             6697\n",
      "46           Riyadh             5288\n",
      "11         Alkhobar              889\n",
      "59          Unknown              840\n",
      "33           Madina              780\n",
      "..              ...              ...\n",
      "1              Afif                2\n",
      "6        Al Khormah                1\n",
      "40          Oyaynah                1\n",
      "47  Riyadh Al-Kabra                1\n",
      "58           Taroot                1\n",
      "\n",
      "[64 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>major_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Riyadh</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Jeddah</td>\n",
       "      <td>Jeddah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Alkhobar</td>\n",
       "      <td>Alkhobar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Makkah</td>\n",
       "      <td>Makkah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95960</th>\n",
       "      <td>Methnab</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108639</th>\n",
       "      <td>Ras Tannura</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112609</th>\n",
       "      <td>Quaieyyah</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152192</th>\n",
       "      <td>Oyaynah</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172095</th>\n",
       "      <td>Riyadh Al-Kabra</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city major_city\n",
       "0                Riyadh     Riyadh\n",
       "12              Unknown      Other\n",
       "16               Jeddah     Jeddah\n",
       "56             Alkhobar   Alkhobar\n",
       "236              Makkah     Makkah\n",
       "...                 ...        ...\n",
       "95960           Methnab      Other\n",
       "108639      Ras Tannura      Other\n",
       "112609        Quaieyyah      Other\n",
       "152192          Oyaynah      Other\n",
       "172095  Riyadh Al-Kabra      Other\n",
       "\n",
       "[64 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'city' and count the number of unique 'unique_id' values\n",
    "unique_id_counts_by_city = primary_data.groupby('city')['unique_id'].nunique().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "unique_id_counts_by_city.columns = ['city', 'unique_id_count']\n",
    "\n",
    "# Print the result\n",
    "print(unique_id_counts_by_city.sort_values('unique_id_count', ascending=False))\n",
    "\n",
    "# Get the top 5 cities by unique_id count\n",
    "top_5_cities = unique_id_counts_by_city.nlargest(6, 'unique_id_count')['city']\n",
    "# Filter out 'Unknown' from the top 5 cities if it exists\n",
    "top_5_cities = top_5_cities[top_5_cities != 'Unknown']\n",
    "\n",
    "# Create the 'major_city' variable\n",
    "primary_data['major_city'] = primary_data['city'].apply(lambda x: x if x in top_5_cities.values else 'Other')\n",
    "\n",
    "cities_table = primary_data[['city', 'major_city']].drop_duplicates()\n",
    "\n",
    "# Print the updated DataFrame\n",
    "cities_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28      JEDDAH\n",
       "46      RIYADH\n",
       "8     ALKHOBAR\n",
       "33      MADINA\n",
       "36      MAKKAH\n",
       "Name: city, dtype: string"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_cities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
